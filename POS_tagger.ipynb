{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech Tagging - Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done by : Bharathi A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import string \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data I have used is from Wall Street Journal. \n",
    "\n",
    "The training data is \"WSJ_train.pos\" and each line of the file contains a particular word and its tag seperated by a tab.\n",
    "\n",
    "The test data is \"WSJ_test.pos\" and it is formatted in the same way as the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = open('WSJ_train.pos','r')\n",
    "\n",
    "f2_test  = open('WSJ_test.pos','r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how the data looks like "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Training data : \n",
      "['In\\tIN\\n', 'an\\tDT\\n', 'Oct.\\tNNP\\n', '19\\tCD\\n', 'review\\tNN\\n', 'of\\tIN\\n', '``\\t``\\n', 'The\\tDT\\n', 'Misanthrope\\tNN\\n', \"''\\t''\\n\"]\n",
      "Sample Test data : \n",
      "['The\\tDT\\n', 'economy\\tNN\\n', \"'s\\tPOS\\n\", 'temperature\\tNN\\n', 'will\\tMD\\n', 'be\\tVB\\n', 'taken\\tVBN\\n', 'from\\tIN\\n', 'several\\tJJ\\n', 'vantage\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "lines_train = f1_train.readlines()\n",
    "'''\n",
    "print(\"Sample Training data : \")\n",
    "print(lines_train[0:10])\n",
    "'''\n",
    "\n",
    "lines_test = f2_test.readlines()\n",
    "'''\n",
    "print(\"Sample Test data : \")\n",
    "print(lines_test[0:10])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above format of data doesn't look easily readable . But , we can understand that each word is separated from its corresponding part of speech tag by a tabspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\tCD\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#A single entry in the dataset\n",
    "\n",
    "#print(lines_train[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "\n",
    "#Rules for tagging unknown terms\n",
    "noun_suffix = [\"action\", \"age\", \"ance\", \"cy\", \"dom\", \"ee\", \"ence\", \"er\", \"hood\", \"ion\", \"ism\", \"ist\", \"ity\", \"ling\", \"ment\", \"ness\", \"or\", \"ry\", \"scape\", \"ship\", \"ty\"]\n",
    "verb_suffix = [\"ate\", \"ify\", \"ise\", \"ize\"]\n",
    "adj_suffix = [\"able\", \"ese\", \"ful\", \"i\", \"ian\", \"ible\", \"ic\", \"ish\", \"ive\", \"less\", \"ly\", \"ous\"]\n",
    "adv_suffix = [\"ward\", \"wards\", \"wise\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells define some utility functions that will be used throughout this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'assign_unk' assigns a 'unk' tag to unknown words and with some prior knowledge , it also assigns some possible additional information about the word such as the word being a verb , noun etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_unk(word):\n",
    "    if any(char.isdigit() for char in word):\n",
    "        return '--unk_digit--'\n",
    "    elif any(char in punct for char in word):\n",
    "        return '--unk_punct--'\n",
    "    elif any(word.endswith(suffix) for suffix in noun_suffix):\n",
    "        return '--unk_noun--'\n",
    "    elif any(word.endswith(suffix) for suffix in verb_suffix):\n",
    "        return '--unk_verb--'\n",
    "    elif any(word.endswith(suffix) for suffix in adj_suffix):\n",
    "        return '--unk_adj--'\n",
    "    elif any(word.endswith(suffix) for suffix in adv_suffix):\n",
    "        return '--unk_adv--'\n",
    "    \n",
    "    return '--unk--'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'get_word_tag_tuple' parses a given line in the training corpus and returns a tuple of (word,tag). If the line was a new line , the word would get a '--n--' tag . \n",
    "\n",
    "If it was an unknown word , it would tag the word using the 'assign_unk' function ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_tag_tuple(line,vocab):\n",
    "    if not line.split():\n",
    "        #It is a new line\n",
    "        word = '--n--'\n",
    "        tag  = '--s--'\n",
    "        return word,tag\n",
    "    else:\n",
    "        word,tag = line.split()\n",
    "        if word not in vocab:\n",
    "            #Tag the word\n",
    "            word = assign_unk(word)\n",
    "        return word,tag\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function 'preprocess' is to preprocess the test set vocabulary(without tags) to mark unknown words(not found in train vocab) and mark end of sentence tokens . \n",
    "\n",
    "This function returns preprocessed_test that contains preprocessed words which will be fed to the testing function later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file,vocab):\n",
    "    \n",
    "    original_test = []\n",
    "    preprocessed_test = []\n",
    "    \n",
    "    fp = open(file,'r')\n",
    "    \n",
    "    for i , word in enumerate(fp):\n",
    "        if not word.strip():\n",
    "            #Assign new line\n",
    "            original_test.append(word.split())\n",
    "            word = '--n--'\n",
    "            preprocessed_test.append(word)\n",
    "            continue\n",
    "        elif word.strip() not in vocab:\n",
    "            original_test.append(word.strip())\n",
    "            #Tag the unknown word\n",
    "            word = assign_unk(word)\n",
    "            preprocessed_test.append(word)\n",
    "            continue\n",
    "        else:\n",
    "            original_test.append(word.strip())\n",
    "            preprocessed_test.append(word.strip())\n",
    "    fp.close()\n",
    "            \n",
    "    assert(len(original_test) == len(open(file, \"r\").readlines()))\n",
    "    assert(len(preprocessed_test) == len(open(file, \"r\").readlines()))\n",
    "            \n",
    "    return original_test , preprocessed_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Training vocabulary from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'an', 'Oct.', '19', 'review', 'of', '``', 'The', 'Misanthrope', \"''\", 'at', 'Chicago', \"'s\", 'Goodman', 'Theatre', '(', '``', 'Revitalized', 'Classics', 'Take']\n"
     ]
    }
   ],
   "source": [
    "#Extract only the word from each line that contains (word,tag)\n",
    "words = [line.split('\\t')[0] for line in lines_train]\n",
    "\n",
    "#print(words[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store in the vocabulary only words that occur twice or more than twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dict to store the freq of words\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for word in words:\n",
    "    word_freq[word]+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the list of words that occur more than once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '.']\n"
     ]
    }
   ],
   "source": [
    "voc = [key for key,value in word_freq.items() if (key!='\\n' and value > 1)]\n",
    "\n",
    "#Sort the words\n",
    "voc = sorted(voc)\n",
    "\n",
    "#print(voc[10:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before writing these words into the file , we also need to append to the vocabulary certain unknown word tags : \n",
    "\n",
    "Eg : --unk_verb-- , --unk_upper etc that will be used to denote Unknown words in the test set / practical scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_tokens = ['--n--','--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--']\n",
    "\n",
    "for token in unknown_tokens : \n",
    "    voc.append(token)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['}', '--n--', '--unk--', '--unk_adj--', '--unk_adv--', '--unk_digit--', '--unk_noun--', '--unk_punct--', '--unk_upper--', '--unk_verb--']\n"
     ]
    }
   ],
   "source": [
    "#print(voc[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the list of words to a file(They form the vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_vocab = open('vocab.txt','w')\n",
    "\n",
    "#Store each word in a newline of the file\n",
    "\n",
    "file_vocab.writelines(\"%s\\n\" % item for item in voc)\n",
    "\n",
    "file_vocab.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23776\n",
      "23776\n"
     ]
    }
   ],
   "source": [
    "assert(len(voc) == len(open('vocab.txt','r').readlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary : \n",
      "['!', '#', '$', '%', '&', \"'\", \"''\", \"'40s\", \"'60s\", \"'70s\", \"'80s\", \"'86\", \"'90s\", \"'N\", \"'S\", \"'d\", \"'em\", \"'ll\", \"'m\", \"'n'\", \"'re\", \"'s\", \"'til\", \"'ve\", '(', ')', ',', '-', '--', '.']\n"
     ]
    }
   ],
   "source": [
    "#Read words from vocab.txt\n",
    "\n",
    "with open('vocab.txt','r') as f:\n",
    "    voc_list = f.read().split('\\n')\n",
    "    \n",
    "#print(\"Vocabulary : \")\n",
    "#print(voc_list[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our Training set vocabulary is created , we have to create the vocabulary for Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', 'vantage', 'points', 'this', 'week', ',', 'with', 'readings', 'on', 'trade', ',', 'output']\n"
     ]
    }
   ],
   "source": [
    "test_words = [line.split('\\t')[0] for line in lines_test]\n",
    "\n",
    "\n",
    "#print(test_words[0:20])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Test vocabulary in a file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is appropriate to store the test set vocabulary, without the tags ,  in a file so that it is easily accessible for further processing and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34199\n"
     ]
    }
   ],
   "source": [
    "#test_words = sorted(test_words)\n",
    "\n",
    "test_file = open('test_vocab.txt','w')\n",
    "\n",
    "#print(len(test_words))\n",
    "\n",
    "\n",
    "test_file.writelines(\"%s\\n\" % item.strip() for item in test_words)\n",
    "test_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Train and Test data for Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Dictionary from Training Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!    :   1\n",
      "#    :   2\n",
      "$    :   3\n",
      "%    :   4\n",
      "&    :   5\n",
      "'    :   6\n",
      "''    :   7\n",
      "'40s    :   8\n",
      "'60s    :   9\n",
      "'70s    :   10\n",
      "'80s    :   11\n",
      "'86    :   12\n",
      "'90s    :   13\n",
      "'N    :   14\n",
      "'S    :   15\n"
     ]
    }
   ],
   "source": [
    "#Build a dictionary vocab that stores the index of each word ( This will be necessary in the Viterbi algorithm)\n",
    "vocab = {}\n",
    "\n",
    "for i , word in enumerate(sorted(voc_list)):\n",
    "    vocab[word] = i\n",
    "    \n",
    "#for word in voc_list[0:15]:\n",
    "    #print(word,\"   :  \",vocab[word])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Test data 'y' and preprocess it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data sample \n",
      "['points\\tNNS\\n', 'this\\tDT\\n', 'week\\tNN\\n', ',\\t,\\n', 'with\\tIN\\n', 'readings\\tNNS\\n', 'on\\tIN\\n', 'trade\\tNN\\n', ',\\t,\\n', 'output\\tNN\\n']\n"
     ]
    }
   ],
   "source": [
    "# 'y' is simply each line from lines_test that contains (word,tag) tuple\n",
    "\n",
    "y = lines_test\n",
    "\n",
    "'''\n",
    "print(\"Test data sample \")\n",
    "print(y[10:20])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocess function is called which returns the preprocessed words for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of Preprocessed Test words : \n",
      "['The', 'economy', \"'s\", 'temperature', 'will', 'be', 'taken', 'from', 'several', '--unk--', 'points', 'this', 'week', ',', 'with', 'readings', 'on', 'trade', ',', 'output']\n"
     ]
    }
   ],
   "source": [
    "_,preprocessed_test = preprocess('test_vocab.txt',vocab)\n",
    "\n",
    "'''\n",
    "print(\"Sample of Preprocessed Test words : \")\n",
    "print(preprocessed_test[0:20])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the Part of speech tagger using Hidden Markov Models , We need three dictionaries :\n",
    "\n",
    "    1. Transition counts - computes the number of times each tag happened next to another tag\n",
    "    2. Emission counts   - computes the probability of a word given its tag.\n",
    "    3. Tag counts        - Computes the number of times each tag appeared in the training set\n",
    "    \n",
    "    These dictionaries will be helpful in computing matrices that are used in actual computation in the Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(training_data,vocab):\n",
    "    '''\n",
    "    Computes and returns the Transition , Emission and Tag counts dictionaries from the raw training data and the vocab dict\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #Initialize empty dictionaries \n",
    "    transition_counts = defaultdict(int)\n",
    "    emission_counts   = defaultdict(int)\n",
    "    tag_counts        = defaultdict(int)\n",
    "    \n",
    "    prev_tag = '--s--'\n",
    "    \n",
    "    for line in training_data:\n",
    "        \n",
    "        word,tag = get_word_tag_tuple(line,vocab)\n",
    "        \n",
    "        transition_counts[(prev_tag,tag)] += 1\n",
    "        \n",
    "        emission_counts[(tag,word)] += 1\n",
    "        \n",
    "        tag_counts[tag] += 1\n",
    "        \n",
    "        prev_tag = tag\n",
    "    \n",
    "    return transition_counts , emission_counts , tag_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_counts , emission_counts , tag_counts = create_dictionaries(lines_train,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample transition counts :\n",
      "(('--s--', 'IN'), 5050)\n",
      "(('IN', 'DT'), 32364)\n",
      "(('DT', 'NNP'), 9044)\n",
      "(('NNP', 'CD'), 1752)\n",
      "\n",
      " Sample Emission counts :\n",
      "(('IN', 'In'), 1735)\n",
      "(('DT', 'an'), 3142)\n",
      "(('NNP', 'Oct.'), 317)\n",
      "(('CD', '19'), 100)\n",
      "\n",
      " Sample Tag counts :\n",
      "('IN', 98554)\n",
      "('DT', 81842)\n",
      "('NNP', 91466)\n",
      "('CD', 36568)\n"
     ]
    }
   ],
   "source": [
    "#Check the values \n",
    "\n",
    "'''\n",
    "print(\"Sample transition counts :\")\n",
    "\n",
    "for item in list(transition_counts.items())[0:4]:\n",
    "    print(item)\n",
    "    \n",
    "print(\"\\n Sample Emission counts :\")\n",
    "\n",
    "for item in list(emission_counts.items())[0:4]:\n",
    "    print(item)\n",
    "    \n",
    "print(\"\\n Sample Tag counts :\")\n",
    "\n",
    "for item in list(tag_counts.items())[0:4]:\n",
    "    print(item)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Probabilities Matrices for Hidden Markov Model computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transition Probability Matrix : Each cell returns the probability to go from one part of speech to another.Here , the POSs act as states of a Markov model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transition_prob_matrix(transition_counts,tag_counts,smoothing_factor):\n",
    "    \n",
    "    tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    n_tags = len(tags)\n",
    "    \n",
    "    #Initialize the matrix TP\n",
    "    \n",
    "    TP = np.zeros((n_tags,n_tags))\n",
    "    \n",
    "    transition_states = set(transition_counts.keys())\n",
    "    \n",
    "    for i in range(n_tags):\n",
    "        \n",
    "        for j in range(n_tags):\n",
    "            \n",
    "            count = 0\n",
    "            \n",
    "            key = (tags[i],tags[j])\n",
    "            \n",
    "            if key in transition_states:\n",
    "                \n",
    "                count = transition_counts[key]\n",
    "            \n",
    "            count_tag_prev = tag_counts[tags[i]]\n",
    "            \n",
    "            TP[i,j] = (count + smoothing_factor)/(count_tag_prev + n_tags*smoothing_factor)\n",
    "    \n",
    "    return TP\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = create_transition_prob_matrix(transition_counts,tag_counts,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emission Probabilities Matrix : The emission probabilities matrix of dimension (n_tags, len(vocab)) computes the emission probability of a (tag,word) in each cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emission_prob_matrix(emission_counts,tag_counts,vocab,smoothing_factor):\n",
    "    \n",
    "    tags = sorted(tag_counts.keys())\n",
    "    \n",
    "    n_tags = len(tag_counts)\n",
    "    \n",
    "    n_words = len(vocab)\n",
    "    \n",
    "    #Initialize empty matrix\n",
    "    E = np.zeros((n_tags,n_words))\n",
    "    \n",
    "    emission_states = set(emission_counts.keys())\n",
    "    \n",
    "    for i in range(n_tags):\n",
    "        \n",
    "        for j in range(n_words):\n",
    "            \n",
    "            count = 0 \n",
    "            \n",
    "            key = (tags[i],vocab[j])\n",
    "            \n",
    "            if key in emission_states:\n",
    "                \n",
    "                count = emission_counts[key]\n",
    "                \n",
    "            count_tag = tag_counts[tags[i]]\n",
    "        \n",
    "            E[i,j] = (count + smoothing_factor)/(count_tag + n_words*smoothing_factor)\n",
    "    \n",
    "    return E\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = create_emission_prob_matrix(emission_counts,tag_counts,list(vocab),0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', '$', \"''\", '(', ')', ',', '--s--', '.', ':', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "states = sorted(tag_counts.keys())\n",
    "\n",
    "#print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store a mapping from Tag abbreviations to their meaningful full forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_mapping ={'CC' : 'conjunction, coordinating',\n",
    "'CD':'cardinal number',\n",
    "'DT' : 'determiner',\n",
    "'EX' : 'existential there',\n",
    "'FW' : 'foreign word',\n",
    "'IN': 'conjunction, subordinating or preposition',\n",
    "'JJ':'adjective',\n",
    "'JJR':'adjective, comparative',\n",
    "'JJS':'adjective, superlative',\n",
    "'LS':'list item marker' ,\n",
    "'MD':'verb, modal auxillary',\n",
    "'NN':'noun, singular or mass',\n",
    "'NNS':'noun, plural',\n",
    "'NNP':'noun, proper singular',\n",
    "'NNPS':'noun, proper plural',\n",
    "'PDT':'predeterminer',\n",
    "'POS':'possessive ending',\n",
    "'PRP':'pronoun, personal',\n",
    "'PRP$':'pronoun, possessive',\n",
    "'RB':'adverb',\n",
    "'RBR':'adverb, comparative',\n",
    "'RBS':'adverb, superlative',\n",
    "'RP':'adverb, particle',\n",
    "'SYM':'symbol',\n",
    "'TO':'infinitival to',\n",
    "'UH':'interjection',\n",
    "'VB':'verb, base form',\n",
    "'VBZ':'verb, 3rd person singular present',\n",
    "'VBP':'verb, non-3rd person singular present',\n",
    "'VBD':'verb, past tense',\n",
    "'VBN':'verb, past participle',\n",
    "'VBG':'verb, gerund or present participle',\n",
    "'WDT':'wh-determiner',\n",
    "'WP':'wh-pronoun, personal',\n",
    "'WP$':'wh-pronoun, possessive',\n",
    "'WRB':'wh-adverb',\n",
    "'.':'punctuation mark, sentence closer',\n",
    "',':'punctuation mark, comma',\n",
    "':':'punctuation mark, colon',\n",
    "'(':'contextual separator, left paren',\n",
    "')':'contextual separator, right paren',\n",
    "'#' : 'unknown word/sentence',\n",
    "'$' : 'unknown word/sentence',\n",
    "\"''\" : \"unknown word/sentence\",\n",
    "'--s--':'start of sentence ',\n",
    "'``' : 'unknown word/sentence'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Viterbi Algorithm for Testing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Viterbi algorithm makes use of Dynamic Programming to find the best sequence of POS tags for a given sequence of words/sentence . It is primarily makes use of two matrices : \n",
    "\n",
    "      1.'best_probs' : Stores the best probabiliy of going from a tag to a word (Hidden state to observable transition)\n",
    "  \n",
    "      2.'best_paths' : Stores the best previous states to reach a particular tag\n",
    "  \n",
    "The Algorithm consists of three main steps :\n",
    " 1. Initialization : For a given corpus/sequence of words , this step initializes the first column of 'best_probs' matrix and initializes the 'best_paths' matrix with zeros . \n",
    " 2. Forward propagation : Populates the two matrices by computing the best probabilities and paths \n",
    " 3. Backward propagation : Traverses backwards through the 'best_paths' matrix to find the best sequence of tags for a given \n",
    "    sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(states,tag_counts,TP,E,test_corpus,vocab):\n",
    "    \n",
    "    n_tags = len(tag_counts)\n",
    "    \n",
    "    #Initialize the two matrices\n",
    "    best_probs = np.zeros((n_tags,len(test_corpus)))\n",
    "    \n",
    "    best_paths = np.zeros((n_tags,len(test_corpus)))\n",
    "    \n",
    "    #Index of a tag denoting the start of the sentence (Before the first word)\n",
    "    start_index = states.index('--s--')\n",
    "    \n",
    "    #Populate first column of 'best_probs' matrix . Go through each POS tag\n",
    "    for i in range(n_tags):\n",
    "        \n",
    "        if TP[start_index,i] == 0:\n",
    "            best_probs[i,0] = float('-inf')\n",
    "            \n",
    "        else:\n",
    "            best_probs[i,0] = math.log(TP[start_index,i]) + math.log(E[i,vocab[test_corpus[0]]])\n",
    "            \n",
    "    return best_probs,best_paths\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_probs , best_paths = initialize(states,tag_counts,TP,E,preprocessed_test,vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to populate the remaining cells in the two matrices by forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(TP,E,test_corpus,best_probs,best_paths,vocab):\n",
    "    \n",
    "    n_tags = best_probs.shape[0]\n",
    "    \n",
    "    #Loop through each word in the test sentence starting from word 1 , since word 0 was already initialized\n",
    "    \n",
    "    for i in range(1,len(test_corpus)):\n",
    "        \n",
    "        for j in range(n_tags):\n",
    "            \n",
    "            temp_best_probs = float('-inf')\n",
    "            \n",
    "            temp_best_paths = None\n",
    "            \n",
    "            for k in range(n_tags):\n",
    "                \n",
    "    \n",
    "                temp_prob = best_probs[k,i-1] + math.log(TP[k,j]) + math.log(E[j,vocab[test_corpus[i]]])\n",
    "                \n",
    "                if temp_prob > temp_best_probs:\n",
    "                    \n",
    "                    temp_best_probs = temp_prob\n",
    "                    \n",
    "                    temp_best_paths = k \n",
    "            \n",
    "            best_probs[j,i] = temp_best_probs\n",
    "            best_paths[j,i] = temp_best_paths\n",
    "    \n",
    "    return best_probs,best_paths\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_probs,best_paths = forward(TP,E,preprocessed_test,best_probs,best_paths,vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function 'backward' retrieves the best sequence of tags through back propagating the 'best_paths' matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(best_probs,best_paths,test_corpus,states):\n",
    "    \n",
    "    #Store the number of words in the test sentence\n",
    "    n_words = best_probs.shape[1]\n",
    "    \n",
    "    n_tags = best_probs.shape[0]\n",
    "    \n",
    "    #Array to store the indices of predicted states\n",
    "    pred_states_index = [None] * n_words\n",
    "    \n",
    "    #Array to store the actual predicted states in string\n",
    "    predictions = [None] * n_words\n",
    "    \n",
    "    best_prob_last_word = float('-inf')\n",
    "    \n",
    "    #Retreive the best probability for the last word and the index of tag for this word \n",
    "    for i in range(best_probs.shape[0]):\n",
    "        \n",
    "        if(best_probs[i,n_words-1]) > best_prob_last_word:\n",
    "            \n",
    "            best_prob_last_word = best_probs[i,n_words-1]\n",
    "            \n",
    "            pred_states_index[n_words - 1] = i\n",
    "            \n",
    "    predictions[n_words-1] = states[pred_states_index[n_words-1]]\n",
    "    \n",
    "    \n",
    "    #Loop backwards through the best_paths matrix from the last word\n",
    "    \n",
    "    for i in range(n_words-1 , -1, -1):\n",
    "        \n",
    "        \n",
    "        tag_word_i = pred_states_index[i]\n",
    "        \n",
    "        pred_states_index[i-1] = best_paths[int(tag_word_i),i]\n",
    "        \n",
    "        predictions[i-1] = states[int(pred_states_index[i-1])]\n",
    "        \n",
    "\n",
    "    \n",
    "    return predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = backward(best_probs,best_paths,preprocessed_test,states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute Accuracy of the Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(predictions,y):\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for prediction , y in zip(predictions,y):\n",
    "        \n",
    "        word_tag = y.split()\n",
    "        \n",
    "        if len(word_tag)!=2:\n",
    "            continue\n",
    "        \n",
    "        word , tag = word_tag\n",
    "        \n",
    "        if tag == prediction :\n",
    "            correct+=1\n",
    "        \n",
    "        total+=1\n",
    "    \n",
    "    return correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the POS Tagger using Viterbi algorithm is : 0.9508720664779472 \n"
     ]
    }
   ],
   "source": [
    "#print(f\"Accuracy of the POS Tagger using Viterbi algorithm is : {get_accuracy(predictions,y)} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Viteri Algorithm acheived about 95% accuracy in the test set  for Parts of Speech Tagging . This can now be applied to test our own example . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to deal with a real-world example , we need to define another function that will preprocess those sentences . The function 'preprocess' defined above only dealt with the particularities of this specific test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test_sample(words,vocab):\n",
    "    \n",
    "    preprocessed_words = []\n",
    "    \n",
    "    for i , word in enumerate(words):\n",
    "        \n",
    "        if not word.strip():\n",
    "            word = '--n--'\n",
    "            preprocessed_words.append(word)\n",
    "            continue\n",
    "        elif word.strip() not in vocab:\n",
    "            word = assign_unk(word)\n",
    "            preprocessed_words.append(word)\n",
    "            continue\n",
    "        else:\n",
    "            preprocessed_words.append(word)\n",
    "    \n",
    "    assert(len(words)==len(preprocessed_words))\n",
    "    \n",
    "    return preprocessed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--unk_punct--']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "sentence = 'sffdhew.'\n",
    "\n",
    "words = sentence.split(' ')\n",
    "\n",
    "preprocessed_words = preprocess_test_sample(words,vocab)\n",
    "\n",
    "print(preprocessed_words)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best_probs , test_best_paths = initialize(states,tag_counts,TP,E,preprocessed_words,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_best_probs , test_best_paths = forward(TP,E,preprocessed_words,test_best_probs,test_best_paths,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_predictions = backward(test_best_probs,test_best_paths,preprocessed_words,states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--unk_punct--        unknown word/sentence\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "for i , pred in enumerate(test_predictions):\n",
    "    print(preprocessed_words[i],'      ',states_mapping[pred])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pickle the Vocabulary , Probability and Count matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part that remains is to pickle important data structures used in this notebook so that they can be referred to in the app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import pickle \n",
    "\n",
    "with open('Transition_probabilities.pickle','wb') as tfp:\n",
    "    pickle.dump(TP,tfp)\n",
    "    \n",
    "with open('Emission probabilities.pickle','wb') as efp:\n",
    "    pickle.dump(E,efp)\n",
    "    \n",
    "with open('states.pickle','wb') as sfp:\n",
    "    pickle.dump(states,sfp)\n",
    "\n",
    "with open('tag_counts.pickle','wb') as tagfp:\n",
    "    pickle.dump(tag_counts,tagfp)\n",
    "    \n",
    "with open('vocab.pickle','wb') as vfp:\n",
    "    pickle.dump(vocab,vfp)\n",
    "    \n",
    "with open('states_mapping.pickle','wb') as smfp:\n",
    "    pickle.dump(states_mapping,smfp)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
